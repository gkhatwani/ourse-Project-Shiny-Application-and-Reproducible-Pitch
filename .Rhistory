doc.corpus <- tm_map(doc.corpus, stripWhitespace)
#If we have a look at what’s left, we find that it’s just the lowercase, stripped down version of the text (which I have truncated here).
inspect(doc.corpus[4])
#Next we create a Term Document Matrix (TDM) which reflects the number of times each word in the corpus is found in each of the documents.
doc.corpus <-tm_map(doc.corpus, PlainTextDocument)
DTM <- DocumentTermMatrix(doc.corpus)
findFreqTerms(DTM, 40)
#Each of these words occurred more that 40 times.
findAssocs(DTM, "definition", 0.4)
#first look at the DTM we know that there are many terms which do not occur very often. It might make sense to simply remove these sparse terms from the analysis.
library(Matrix)
mat <- sparseMatrix(i=DTM$i, j=DTM$j, x=DTM$v,
dims=c(DTM$nrow, DTM$ncol))
mainclasses<-as.factor(mainclasses)
subclasses<-as.factor(subclasses)
mainclasstrain<-data.frame(mainclasses,subclasses)
ncoltest<-dim(mat)[2]
for(i in 1:ncoltest){ mainclasstrain<-data.frame(mainclasstrain,as.factor(mat[,i]))}
colnames(mainclasstrain)[3:(ncoltest+2)]<-DTM$dimnames$Terms
library(e1071)
test2<-mainclasstrain[1:(nrow(mainclasstrain)-1),]
mainclassmodel<-naiveBayes(mainclasses ~ . -subclasses, data = test2)
predictedmainclass<-predict(mainclassmodel, mainclasstrain[dim(mat)[1],])
subclasstrain<-test2[mainclasses==predictedmainclass,]
subclassmodel<-naiveBayes(subclasses ~ . -mainclasses, data = subclasstrain)
predictedsubclass<-predict(subclassmodel, mainclasstrain[dim(mat)[1],])
answer<-paste0(predictedmainclass,":",predictedsubclass)
answer
query=""
file="http://cogcomp.cs.illinois.edu/Data/QA/QC/train_1000.label"
library(stringr)
download.file(file,destfile = "./data/data.txt")
temp<-readLines("./data/data.txt")
mainclasses<-str_trim(gsub(":(.*)+"," ",temp))
mainclasses<-c(mainclasses,"")
subdata<-sapply(strsplit(temp, split=':', fixed=TRUE), function(x) (x[2]))
subclasses<-sapply(strsplit(subdata, split=' ', fixed=TRUE), function(x) (x[1]))
subclasses<-c(subclasses,"")
temp<-str_trim(sub("(.*?) "," ",temp))
temp<-c(temp,query)
#The next task was to convert these documents into a corpus.
library(tm)
doc.vec <- VectorSource(temp)
doc.corpus <- Corpus(doc.vec)
#There is a lot of information in those documents which is not particularly useful for text mining. So before proceeding any further, we will clean things up a bit. First we convert all of the text to lowercase and then remove punctuation, numbers and common English stopwords. Possibly the list of English stop words is not entirely appropriate for Shakespearean English, but it is a reasonable starting point.
doc.corpus <-tm_map(doc.corpus, PlainTextDocument)
doc.corpus <- tm_map(doc.corpus, tolower)
doc.corpus <- tm_map(doc.corpus, removePunctuation)
doc.corpus <- tm_map(doc.corpus, removeNumbers)
doc.corpus <- tm_map(doc.corpus, removeWords, stopwords("english"))
Next we perform stemming, which removes affixes from words (so, for example, “run”, “runs” and “running” all become “run”).
library(SnowballC)
doc.corpus <- tm_map(doc.corpus, stemDocument)
#All of these transformations have resulted in a lot of whitespace, which is then removed.
doc.corpus <- tm_map(doc.corpus, stripWhitespace)
#If we have a look at what’s left, we find that it’s just the lowercase, stripped down version of the text (which I have truncated here).
inspect(doc.corpus[4])
#Next we create a Term Document Matrix (TDM) which reflects the number of times each word in the corpus is found in each of the documents.
doc.corpus <-tm_map(doc.corpus, PlainTextDocument)
DTM <- DocumentTermMatrix(doc.corpus)
findFreqTerms(DTM, 40)
#Each of these words occurred more that 40 times.
findAssocs(DTM, "definition", 0.4)
#first look at the DTM we know that there are many terms which do not occur very often. It might make sense to simply remove these sparse terms from the analysis.
library(Matrix)
mat <- sparseMatrix(i=DTM$i, j=DTM$j, x=DTM$v,
dims=c(DTM$nrow, DTM$ncol))
mainclasses<-as.factor(mainclasses)
subclasses<-as.factor(subclasses)
mainclasstrain<-data.frame(mainclasses,subclasses)
ncoltest<-dim(mat)[2]
for(i in 1:ncoltest){ mainclasstrain<-data.frame(mainclasstrain,as.factor(mat[,i]))}
colnames(mainclasstrain)[3:(ncoltest+2)]<-DTM$dimnames$Terms
library(e1071)
test2<-mainclasstrain[1:(nrow(mainclasstrain)-1),]
mainclassmodel<-naiveBayes(mainclasses ~ . -subclasses, data = test2)
predictedmainclass<-predict(mainclassmodel, mainclasstrain[dim(mat)[1],])
subclasstrain<-test2[mainclasses==predictedmainclass,]
subclassmodel<-naiveBayes(subclasses ~ . -mainclasses, data = subclasstrain)
predictedsubclass<-predict(subclassmodel, mainclasstrain[dim(mat)[1],])
answer<-paste0(predictedmainclass,":",predictedsubclass)
if(names(table(test[dim(mat)[1],3:(ncoltest+2)]==0))[1]=="TRUE"){ print("Unknown")}else{ print(answer)}
if(names(table(mainclasstrain[dim(mat)[1],3:(ncoltest+2)]==0))[1]=="TRUE"){ print("Unknown")}else{ print(answer)}
queans<-function(query="",file="http://cogcomp.cs.illinois.edu/Data/QA/QC/train_1000.label")
{
library(stringr)
download.file(file,destfile = "./data/data.txt")
temp<-readLines("./data/data.txt")
mainclasses<-str_trim(gsub(":(.*)+"," ",temp))
mainclasses<-c(mainclasses,"")
subdata<-sapply(strsplit(temp, split=':', fixed=TRUE), function(x) (x[2]))
subclasses<-sapply(strsplit(subdata, split=' ', fixed=TRUE), function(x) (x[1]))
subclasses<-c(subclasses,"")
temp<-str_trim(sub("(.*?) "," ",temp))
temp<-c(temp,query)
#The next task was to convert these documents into a corpus.
library(tm)
doc.vec <- VectorSource(temp)
doc.corpus <- Corpus(doc.vec)
#There is a lot of information in those documents which is not particularly useful for text mining. So before proceeding any further, we will clean
things up a bit. First we convert all of the text to lowercase and then remove punctuation, numbers and common English stopwords. Possibly the list
of English stop words is not entirely appropriate for Shakespearean English, but it is a reasonable starting point.
doc.corpus <-tm_map(doc.corpus, PlainTextDocument)
doc.corpus <- tm_map(doc.corpus, tolower)
doc.corpus <- tm_map(doc.corpus, removePunctuation)
doc.corpus <- tm_map(doc.corpus, removeNumbers)
doc.corpus <- tm_map(doc.corpus, removeWords, stopwords("english"))
Next we perform stemming, which removes affixes from words (so, for example, “run”, “runs” and “running” all become “run”).
library(SnowballC)
doc.corpus <- tm_map(doc.corpus, stemDocument)
#All of these transformations have resulted in a lot of whitespace, which is then removed.
doc.corpus <- tm_map(doc.corpus, stripWhitespace)
#If we have a look at what’s left, we find that it’s just the lowercase, stripped down version of the text (which I have truncated here).
inspect(doc.corpus[4])
#Next we create a Term Document Matrix (TDM) which reflects the number of times each word in the corpus is found in each of the documents.
doc.corpus <-tm_map(doc.corpus, PlainTextDocument)
DTM <- DocumentTermMatrix(doc.corpus)
findFreqTerms(DTM, 40)
#Each of these words occurred more that 40 times.
findAssocs(DTM, "definition", 0.4)
#first look at the DTM we know that there are many terms which do not occur very often. It might make sense to simply remove these sparse terms from
the analysis.
library(Matrix)
mat <- sparseMatrix(i=DTM$i, j=DTM$j, x=DTM$v,
dims=c(DTM$nrow, DTM$ncol))
mainclasses<-as.factor(mainclasses)
subclasses<-as.factor(subclasses)
mainclasstrain<-data.frame(mainclasses,subclasses)
ncoltest<-dim(mat)[2]
for(i in 1:ncoltest){ mainclasstrain<-data.frame(mainclasstrain,as.factor(mat[,i]))}
colnames(mainclasstrain)[3:(ncoltest+2)]<-DTM$dimnames$Terms
library(e1071)
test2<-mainclasstrain[1:(nrow(mainclasstrain)-1),]
mainclassmodel<-naiveBayes(mainclasses ~ . -subclasses, data = test2)
predictedmainclass<-predict(mainclassmodel, mainclasstrain[dim(mat)[1],])
subclasstrain<-test2[mainclasses==predictedmainclass,]
subclassmodel<-naiveBayes(subclasses ~ . -mainclasses, data = subclasstrain)
predictedsubclass<-predict(subclassmodel, mainclasstrain[dim(mat)[1],])
answer<-paste0(predictedmainclass,":",predictedsubclass)
if(names(table(mainclasstrain[dim(mat)[1],3:(ncoltest+2)]==0))[1]=="TRUE"){ print("Unknown")}else{ print(answer)}
}
library(stringr)
download.file(file,destfile = "./data/data.txt")
temp<-readLines("./data/data.txt")
mainclasses<-str_trim(gsub(":(.*)+"," ",temp))
mainclasses<-c(mainclasses,"")
subdata<-sapply(strsplit(temp, split=':', fixed=TRUE), function(x) (x[2]))
subclasses<-sapply(strsplit(subdata, split=' ', fixed=TRUE), function(x) (x[1]))
subclasses<-c(subclasses,"")
temp<-str_trim(sub("(.*?) "," ",temp))
temp<-c(temp,query)
#The next task was to convert these documents into a corpus.
library(tm)
doc.vec <- VectorSource(temp)
doc.corpus <- Corpus(doc.vec)
#There is a lot of information in those documents which is not particularly useful for text mining. So before proceeding any further, we will clean
things up a bit. First we convert all of the text to lowercase and then remove punctuation, numbers and common English stopwords. Possibly the list
of English stop words is not entirely appropriate for Shakespearean English, but it is a reasonable starting point.
doc.corpus <-tm_map(doc.corpus, PlainTextDocument)
doc.corpus <- tm_map(doc.corpus, tolower)
doc.corpus <- tm_map(doc.corpus, removePunctuation)
doc.corpus <- tm_map(doc.corpus, removeNumbers)
doc.corpus <- tm_map(doc.corpus, removeWords, stopwords("english"))
Next we perform stemming, which removes affixes from words (so, for example, “run”, “runs” and “running” all become “run”).
library(SnowballC)
doc.corpus <- tm_map(doc.corpus, stemDocument)
#All of these transformations have resulted in a lot of whitespace, which is then removed.
doc.corpus <- tm_map(doc.corpus, stripWhitespace)
#If we have a look at what’s left, we find that it’s just the lowercase, stripped down version of the text (which I have truncated here).
inspect(doc.corpus[4])
#Next we create a Term Document Matrix (TDM) which reflects the number of times each word in the corpus is found in each of the documents.
doc.corpus <-tm_map(doc.corpus, PlainTextDocument)
DTM <- DocumentTermMatrix(doc.corpus)
findFreqTerms(DTM, 40)
#Each of these words occurred more that 40 times.
findAssocs(DTM, "definition", 0.4)
#first look at the DTM we know that there are many terms which do not occur very often. It might make sense to simply remove these sparse terms from
the analysis.
library(Matrix)
mat <- sparseMatrix(i=DTM$i, j=DTM$j, x=DTM$v,
dims=c(DTM$nrow, DTM$ncol))
mainclasses<-as.factor(mainclasses)
subclasses<-as.factor(subclasses)
mainclasstrain<-data.frame(mainclasses,subclasses)
ncoltest<-dim(mat)[2]
for(i in 1:ncoltest){ mainclasstrain<-data.frame(mainclasstrain,as.factor(mat[,i]))}
colnames(mainclasstrain)[3:(ncoltest+2)]<-DTM$dimnames$Terms
library(e1071)
test2<-mainclasstrain[1:(nrow(mainclasstrain)-1),]
mainclassmodel<-naiveBayes(mainclasses ~ . -subclasses, data = test2)
predictedmainclass<-predict(mainclassmodel, mainclasstrain[dim(mat)[1],])
subclasstrain<-test2[mainclasses==predictedmainclass,]
subclassmodel<-naiveBayes(subclasses ~ . -mainclasses, data = subclasstrain)
predictedsubclass<-predict(subclassmodel, mainclasstrain[dim(mat)[1],])
answer<-paste0(predictedmainclass,":",predictedsubclass)
if(names(table(mainclasstrain[dim(mat)[1],3:(ncoltest+2)]==0))[1]=="TRUE"){ print("Unknown")}else{ print(answer)}
names(table(mainclasstrain[dim(mat)[1],3:(ncoltest+2)]==0))[1]=="TRUE"
names(table(mainclasstrain[dim(mat)[1],3:(ncoltest+2)]==0))[1]
table(mainclasstrain[dim(mat)[1],3:(ncoltest+2)]
table(mainclasstrain[dim(mat)[1],3:(ncoltest+2)])
table(mainclasstrain[dim(mat)[1],3:(ncoltest+2)])\
table(mainclasstrain[dim(mat)[1],3:(ncoltest+2)])
dim(mat)[1]
dim(mat)
names(table(mainclasstrain[dim(mat)[1],3:(ncoltest+2)]==0))[1]=="TRUE"
names(table(mainclasstrain[dim(mat)[1],3:(ncoltest+2)]==0))
table(mainclasstrain[dim(mat)[1],3:(ncoltest+2)]==0)
table(mainclasstrain[23,3:(ncoltest+2)]==0)
queans<-function(query="",file="http://cogcomp.cs.illinois.edu/Data/QA/QC/train_1000.label")
{
library(stringr)
download.file(file,destfile = "./data/data.txt")
temp<-readLines("./data/data.txt")
mainclasses<-str_trim(gsub(":(.*)+"," ",temp))
mainclasses<-c(mainclasses,"")
subdata<-sapply(strsplit(temp, split=':', fixed=TRUE), function(x) (x[2]))
subclasses<-sapply(strsplit(subdata, split=' ', fixed=TRUE), function(x) (x[1]))
subclasses<-c(subclasses,"")
temp<-str_trim(sub("(.*?) "," ",temp))
temp<-c(temp,query)
#The next task was to convert these documents into a corpus.
library(tm)
doc.vec <- VectorSource(temp)
doc.corpus <- Corpus(doc.vec)
#There is a lot of information in those documents which is not particularly useful for text mining. So before proceeding any further, we will clean
things up a bit. First we convert all of the text to lowercase and then remove punctuation, numbers and common English stopwords. Possibly the list
of English stop words is not entirely appropriate for Shakespearean English, but it is a reasonable starting point.
doc.corpus <-tm_map(doc.corpus, PlainTextDocument)
doc.corpus <- tm_map(doc.corpus, tolower)
doc.corpus <- tm_map(doc.corpus, removePunctuation)
doc.corpus <- tm_map(doc.corpus, removeNumbers)
doc.corpus <- tm_map(doc.corpus, removeWords, stopwords("english"))
Next we perform stemming, which removes affixes from words (so, for example, “run”, “runs” and “running” all become “run”).
library(SnowballC)
doc.corpus <- tm_map(doc.corpus, stemDocument)
#All of these transformations have resulted in a lot of whitespace, which is then removed.
doc.corpus <- tm_map(doc.corpus, stripWhitespace)
#If we have a look at what’s left, we find that it’s just the lowercase, stripped down version of the text (which I have truncated here).
inspect(doc.corpus[4])
#Next we create a Term Document Matrix (TDM) which reflects the number of times each word in the corpus is found in each of the documents.
doc.corpus <-tm_map(doc.corpus, PlainTextDocument)
DTM <- DocumentTermMatrix(doc.corpus)
findFreqTerms(DTM, 40)
#Each of these words occurred more that 40 times.
findAssocs(DTM, "definition", 0.4)
#first look at the DTM we know that there are many terms which do not occur very often. It might make sense to simply remove these sparse terms from
the analysis.
library(Matrix)
mat <- sparseMatrix(i=DTM$i, j=DTM$j, x=DTM$v,
dims=c(DTM$nrow, DTM$ncol))
mainclasses<-as.factor(mainclasses)
subclasses<-as.factor(subclasses)
mainclasstrain<-data.frame(mainclasses,subclasses)
ncoltest<-dim(mat)[2]
for(i in 1:ncoltest){ mainclasstrain<-data.frame(mainclasstrain,as.factor(mat[,i]))}
colnames(mainclasstrain)[3:(ncoltest+2)]<-DTM$dimnames$Terms
library(e1071)
test2<-mainclasstrain[1:(nrow(mainclasstrain)-1),]
mainclassmodel<-naiveBayes(mainclasses ~ . -subclasses, data = test2)
predictedmainclass<-predict(mainclassmodel, mainclasstrain[dim(mat)[1],])
subclasstrain<-test2[mainclasses==predictedmainclass,]
subclassmodel<-naiveBayes(subclasses ~ . -mainclasses, data = subclasstrain)
predictedsubclass<-predict(subclassmodel, mainclasstrain[dim(mat)[1],])
answer<-paste0(predictedmainclass,":",predictedsubclass)
if(names(table(mainclasstrain[dim(mat)[1],3:(ncoltest+2)]==0))[1]=="TRUE"){ return("Unknown")}else{ return(answer)}
}
mysummary <- function(x,npar=TRUE,print=TRUE) {
if (!npar) {
center <- mean(x); spread <- sd(x)
} else {
center <- median(x); spread <- mad(x)
}
if (print & !npar) {
cat("Mean=", center, "\n", "SD=", spread, "\n")
} else if (print & npar) {
cat("Median=", center, "\n", "MAD=", spread, "\n")
}
result <- list(center=center,spread=spread)
return(result)
}
answer<-"unknown"
answer
queans <- function(query="",file="http://cogcomp.cs.illinois.edu/Data/QA/QC/train_1000.label"){
library(stringr)
download.file(file,destfile = "./data/data.txt")
temp<-readLines("./data/data.txt")
mainclasses<-str_trim(gsub(":(.*)+"," ",temp))
mainclasses<-c(mainclasses,"")
subdata<-sapply(strsplit(temp, split=':', fixed=TRUE), function(x) (x[2]))
subclasses<-sapply(strsplit(subdata, split=' ', fixed=TRUE), function(x) (x[1]))
subclasses<-c(subclasses,"")
temp<-str_trim(sub("(.*?) "," ",temp))
temp<-c(temp,query)
#The next task was to convert these documents into a corpus.
library(tm)
doc.vec <- VectorSource(temp)
doc.corpus <- Corpus(doc.vec)
#There is a lot of information in those documents which is not particularly useful for text mining. So before proceeding any further, we will clean
things up a bit. First we convert all of the text to lowercase and then remove punctuation, numbers and common English stopwords. Possibly the list
of English stop words is not entirely appropriate for Shakespearean English, but it is a reasonable starting point.
doc.corpus <-tm_map(doc.corpus, PlainTextDocument)
doc.corpus <- tm_map(doc.corpus, tolower)
doc.corpus <- tm_map(doc.corpus, removePunctuation)
doc.corpus <- tm_map(doc.corpus, removeNumbers)
doc.corpus <- tm_map(doc.corpus, removeWords, stopwords("english"))
Next we perform stemming, which removes affixes from words (so, for example, “run”, “runs” and “running” all become “run”).
library(SnowballC)
doc.corpus <- tm_map(doc.corpus, stemDocument)
#All of these transformations have resulted in a lot of whitespace, which is then removed.
doc.corpus <- tm_map(doc.corpus, stripWhitespace)
#If we have a look at what’s left, we find that it’s just the lowercase, stripped down version of the text (which I have truncated here).
inspect(doc.corpus[4])
#Next we create a Term Document Matrix (TDM) which reflects the number of times each word in the corpus is found in each of the documents.
doc.corpus <-tm_map(doc.corpus, PlainTextDocument)
DTM <- DocumentTermMatrix(doc.corpus)
findFreqTerms(DTM, 40)
#Each of these words occurred more that 40 times.
findAssocs(DTM, "definition", 0.4)
#first look at the DTM we know that there are many terms which do not occur very often. It might make sense to simply remove these sparse terms from
the analysis.
library(Matrix)
mat <- sparseMatrix(i=DTM$i, j=DTM$j, x=DTM$v,
dims=c(DTM$nrow, DTM$ncol))
mainclasses<-as.factor(mainclasses)
subclasses<-as.factor(subclasses)
mainclasstrain<-data.frame(mainclasses,subclasses)
ncoltest<-dim(mat)[2]
for(i in 1:ncoltest){ mainclasstrain<-data.frame(mainclasstrain,as.factor(mat[,i]))}
colnames(mainclasstrain)[3:(ncoltest+2)]<-DTM$dimnames$Terms
library(e1071)
test2<-mainclasstrain[1:(nrow(mainclasstrain)-1),]
mainclassmodel<-naiveBayes(mainclasses ~ . -subclasses, data = test2)
predictedmainclass<-predict(mainclassmodel, mainclasstrain[dim(mat)[1],])
subclasstrain<-test2[mainclasses==predictedmainclass,]
subclassmodel<-naiveBayes(subclasses ~ . -mainclasses, data = subclasstrain)
predictedsubclass<-predict(subclassmodel, mainclasstrain[dim(mat)[1],])
answer<-paste0(predictedmainclass,":",predictedsubclass)
if(names(table(mainclasstrain[dim(mat)[1],3:(ncoltest+2)]==0))[1]=="TRUE"){ answer<-"Unknown"}else{ answer<-answer}
return(answer)
}
queans <- function(query="",file="http://cogcomp.cs.illinois.edu/Data/QA/QC/train_1000.label"){
library(stringr)
download.file(file,destfile = "./data/data.txt")
temp<-readLines("./data/data.txt")
mainclasses<-str_trim(gsub(":(.*)+"," ",temp))
mainclasses<-c(mainclasses,"")
subdata<-sapply(strsplit(temp, split=':', fixed=TRUE), function(x) (x[2]))
subclasses<-sapply(strsplit(subdata, split=' ', fixed=TRUE), function(x) (x[1]))
subclasses<-c(subclasses,"")
temp<-str_trim(sub("(.*?) "," ",temp))
temp<-c(temp,query)
#The next task was to convert these documents into a corpus.
library(tm)
doc.vec <- VectorSource(temp)
doc.corpus <- Corpus(doc.vec)
#There is a lot of information in those documents which is not particularly useful for text mining. So before proceeding any further, we will clean
things up a bit. First we convert all of the text to lowercase and then remove punctuation, numbers and common English stopwords. Possibly the list
of English stop words is not entirely appropriate for Shakespearean English, but it is a reasonable starting point.
doc.corpus <-tm_map(doc.corpus, PlainTextDocument)
doc.corpus <- tm_map(doc.corpus, tolower)
doc.corpus <- tm_map(doc.corpus, removePunctuation)
doc.corpus <- tm_map(doc.corpus, removeNumbers)
doc.corpus <- tm_map(doc.corpus, removeWords, stopwords("english"))
Next we perform stemming, which removes affixes from words (so, for example, “run”, “runs” and “running” all become “run”).
library(SnowballC)
doc.corpus <- tm_map(doc.corpus, stemDocument)
#All of these transformations have resulted in a lot of whitespace, which is then removed.
doc.corpus <- tm_map(doc.corpus, stripWhitespace)
#If we have a look at what’s left, we find that it’s just the lowercase, stripped down version of the text (which I have truncated here).
inspect(doc.corpus[4])
#Next we create a Term Document Matrix (TDM) which reflects the number of times each word in the corpus is found in each of the documents.
doc.corpus <-tm_map(doc.corpus, PlainTextDocument)
DTM <- DocumentTermMatrix(doc.corpus)
findFreqTerms(DTM, 40)
#Each of these words occurred more that 40 times.
findAssocs(DTM, "definition", 0.4)
#first look at the DTM we know that there are many terms which do not occur very often. It might make sense to simply remove these sparse terms from
the analysis.
library(Matrix)
mat <- sparseMatrix(i=DTM$i, j=DTM$j, x=DTM$v,
dims=c(DTM$nrow, DTM$ncol))
mainclasses<-as.factor(mainclasses)
subclasses<-as.factor(subclasses)
mainclasstrain<-data.frame(mainclasses,subclasses)
ncoltest<-dim(mat)[2]
for(i in 1:ncoltest){ mainclasstrain<-data.frame(mainclasstrain,as.factor(mat[,i]))}
colnames(mainclasstrain)[3:(ncoltest+2)]<-DTM$dimnames$Terms
library(e1071)
test2<-mainclasstrain[1:(nrow(mainclasstrain)-1),]
mainclassmodel<-naiveBayes(mainclasses ~ . -subclasses, data = test2)
predictedmainclass<-predict(mainclassmodel, mainclasstrain[dim(mat)[1],])
subclasstrain<-test2[mainclasses==predictedmainclass,]
subclassmodel<-naiveBayes(subclasses ~ . -mainclasses, data = subclasstrain)
predictedsubclass<-predict(subclassmodel, mainclasstrain[dim(mat)[1],])
answer<-paste0(predictedmainclass,":",predictedsubclass)
if(names(table(mainclasstrain[dim(mat)[1],3:(ncoltest+2)]==0))[1]=="TRUE"){ answer<-"Unknown"} else if{ answer<-answer}
return(answer)
}
queans <- function(query="",file="http://cogcomp.cs.illinois.edu/Data/QA/QC/train_1000.label"){
library(stringr)
download.file(file,destfile = "./data/data.txt")
temp<-readLines("./data/data.txt")
mainclasses<-str_trim(gsub(":(.*)+"," ",temp))
mainclasses<-c(mainclasses,"")
subdata<-sapply(strsplit(temp, split=':', fixed=TRUE), function(x) (x[2]))
subclasses<-sapply(strsplit(subdata, split=' ', fixed=TRUE), function(x) (x[1]))
subclasses<-c(subclasses,"")
temp<-str_trim(sub("(.*?) "," ",temp))
temp<-c(temp,query)
#The next task was to convert these documents into a corpus.
library(tm)
doc.vec <- VectorSource(temp)
doc.corpus <- Corpus(doc.vec)
# clean things up a bit. convert all of the text to lowercase and then remove punctuation, numbers and common English stopwords.
doc.corpus <-tm_map(doc.corpus, PlainTextDocument)
doc.corpus <- tm_map(doc.corpus, tolower)
doc.corpus <- tm_map(doc.corpus, removePunctuation)
doc.corpus <- tm_map(doc.corpus, removeNumbers)
doc.corpus <- tm_map(doc.corpus, removeWords, stopwords("english"))
#perform stemming, which removes affixes from words .
library(SnowballC)
doc.corpus <- tm_map(doc.corpus, stemDocument)
#All of these transformations have resulted in a lot of whitespace, which is then removed.
doc.corpus <- tm_map(doc.corpus, stripWhitespace)
inspect(doc.corpus[4])
#Next we create a Term Document Matrix (TDM) which reflects the number of times each word in the corpus is found in each of the documents.
doc.corpus <-tm_map(doc.corpus, PlainTextDocument)
DTM <- DocumentTermMatrix(doc.corpus)
findFreqTerms(DTM, 40)
#Each of these words occurred more that 40 times.
findAssocs(DTM, "definition", 0.4)
library(Matrix)
mat <- sparseMatrix(i=DTM$i, j=DTM$j, x=DTM$v,
dims=c(DTM$nrow, DTM$ncol))
mainclasses<-as.factor(mainclasses)
subclasses<-as.factor(subclasses)
mainclasstrain<-data.frame(mainclasses,subclasses)
ncoltest<-dim(mat)[2]
for(i in 1:ncoltest){ mainclasstrain<-data.frame(mainclasstrain,as.factor(mat[,i]))}
colnames(mainclasstrain)[3:(ncoltest+2)]<-DTM$dimnames$Terms
library(e1071)
test2<-mainclasstrain[1:(nrow(mainclasstrain)-1),]
mainclassmodel<-naiveBayes(mainclasses ~ . -subclasses, data = test2)
predictedmainclass<-predict(mainclassmodel, mainclasstrain[dim(mat)[1],])
subclasstrain<-test2[mainclasses==predictedmainclass,]
subclassmodel<-naiveBayes(subclasses ~ . -mainclasses, data = subclasstrain)
predictedsubclass<-predict(subclassmodel, mainclasstrain[dim(mat)[1],])
answer<-paste0(predictedmainclass,":",predictedsubclass)
if(names(table(mainclasstrain[dim(mat)[1],3:(ncoltest+2)]==0))[1]=="TRUE"){ answer<-"Unknown"} else { answer<-answer}
return(answer)
}
queans()
queans("Which two states enclose Chesapeake Bay ?")
install.packages("RMySQL")
install.packages("RMySQL")
install.packages("RMySQL")
library(RMySQL)
find_rtools()
install.packages("KernSmooth")
library(KernSmooth)
install.packages("devtools")
install.packages("shinyapps")
install.packages("shinyapps")
install.packages("shiny")
library(shiny)
runApp()
getwd()
setwd
setwd("C:\\Users\\hp\\Desktop\\Tanuj")
setwd
runApp()
runApp()
runApp()
install.packages("slidify")
install_github('slidify', 'ramnathv')
library(devtools)
install_github('slidify', 'ramnathv')
install_github('slidify', 'ramnathv/slidify')
install_github('slidify', 'ramnathv/slidify/slidify')
install_github('slidify', 'ramnathv')
available.packages(slidify)
available.packages("slidify")
available.packages(slidify)
is.package("slidify")
?is.package
setwd("C:\\Users\\hp\\Desktop\\Tanuj\\ddpCourseProject-master\\ddpCourseProject-master")
runApp()
install_github('rstudio/shinyapps')
install.packages("digest")
install.packages("digest")
install.packages("packrat")
install_github('rstudio/shinyapps')
install_github('rstudio/shinyapps')
install_github('rstudio/shinyapps')
install_github('rstudio/shinyapps')
library(devtools\)
library(devtools)
install_github('rstudio/shinyapps')
library(shinyapps)
install.packages('rsconnect')
rsconnect::setAccountInfo(name='gkhatwani',
token='969516270EC743D527AF6383E8643724',
secret='0oBefgceMBmiNZWkm9WbFoQDeFhyYAYZ6r3vsoGi')
library(rsconnect)
rsconnect::deployApp('path/to/your/app')
library(rsconnect)
rsconnect::deployApp('https://gkhatwani.shinyapps.io/predictor')
library(shiny)
runApp()
runApp()
runApp()
runApp()
